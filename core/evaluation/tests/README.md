# Tests de RegresiÃ³n

Este directorio contiene tests de regresiÃ³n para asegurar que la nueva implementaciÃ³n `grapheval` produce los mismos resultados que la implementaciÃ³n original `rdfeval`.

## ğŸ“ Estructura

```
tests/
â”œâ”€â”€ README.md              # Este archivo
â”œâ”€â”€ generate_baseline.py   # Genera mÃ©tricas de referencia con rdfeval (antiguo)
â”œâ”€â”€ test_regression.py     # Compara grapheval (nuevo) vs baselines
â””â”€â”€ baselines/             # Resultados de referencia (generados automÃ¡ticamente)
    â”œâ”€â”€ baseline_gold_pred.json
    â””â”€â”€ baseline_gold2_pred.json
```

## ğŸš€ Uso

### 1. Generar Baselines (Primera vez)

Primero, genera los resultados de referencia usando la implementaciÃ³n antigua:

```bash
python tests/generate_baseline.py
```

Esto crearÃ¡ archivos JSON en `tests/baselines/` con los resultados de la implementaciÃ³n antigua (`rdfeval`) para:
- `data/pred.nt` vs `data/gold.nt`
- `data/pred.nt` vs `data/gold2.nt`

### 2. Ejecutar Tests de RegresiÃ³n

Luego, ejecuta los tests que comparan la nueva implementaciÃ³n contra los baselines:

```bash
python tests/test_regression.py
```

Esto ejecutarÃ¡ la nueva implementaciÃ³n (`grapheval`) y compararÃ¡ los resultados mÃ©trica por mÃ©trica.

### 3. Interpretar Resultados

El script mostrarÃ¡:
- âœ… **Verde**: MÃ©tricas que coinciden exactamente
- âŒ **Rojo**: MÃ©tricas que difieren
- âš ï¸ **Amarillo**: MÃ©tricas faltantes

Ejemplo de salida:

```
âœ… triples.precision: 0.8571
âœ… triples.recall: 0.8571
âœ… triples.f1: 0.8571
âœ… subjects.precision: 1.0000
âœ… subjects.recall: 1.0000
âœ… subjects.f1: 1.0000

SUMMARY
=========================================
Overall:
  Total metrics compared: 48
  âœ… ALL TESTS PASSED! (48/48)
```

## ğŸ” QuÃ© se Compara

Los tests comparan las siguientes mÃ©tricas para cada categorÃ­a:

### MÃ©tricas BÃ¡sicas
- **triples**: EvaluaciÃ³n de triples completos
- **subjects**: EvaluaciÃ³n de sujetos
- **subjects_fuzzy**: EvaluaciÃ³n fuzzy de sujetos
- **classes**: EvaluaciÃ³n de clases
- **classes_unique**: EvaluaciÃ³n de clases Ãºnicas

### MÃ©tricas de Propiedades
- **predicates**: EvaluaciÃ³n de predicados
- **predicates_unique**: EvaluaciÃ³n de predicados Ãºnicos
- **predicate_datatype_range**: EvaluaciÃ³n de datatypes
- **predicate_datatype_range_unique**: EvaluaciÃ³n Ãºnica de datatypes

### MÃ©tricas de Objetos
- **objects**: EvaluaciÃ³n de objetos
- **objects_uris**: EvaluaciÃ³n de URIs
- **objects_literals**: EvaluaciÃ³n de literales

### Valores Comparados

Para cada mÃ©trica se comparan:
- `tp`: True Positives
- `fp`: False Positives
- `fn`: False Negatives
- `tn`: True Negatives
- `precision`: PrecisiÃ³n
- `recall`: Recall
- `f1`: F1-Score

## ğŸ› ï¸ Troubleshooting

### Error: "Baseline not found"

Si ves este error:
```
âŒ Baseline not found: tests/baselines/baseline_gold_pred.json
   Run: python tests/generate_baseline.py
```

**SoluciÃ³n**: Ejecuta primero el script de generaciÃ³n de baselines:
```bash
python tests/generate_baseline.py
```

### Error al importar rdfeval

Si al generar baselines obtienes un error de importaciÃ³n:
```
ImportError: cannot import name 'RDFeval' from 'rdfeval.rdfeval'
```

**SoluciÃ³n**: AsegÃºrate de que el paquete antiguo `rdfeval` estÃ¡ disponible. Si ya lo eliminaste, puedes regenerar los baselines mÃ¡s tarde o usar los que ya tienes guardados.

### Diferencias en los resultados

Si encuentras diferencias:

1. **Diferencias pequeÃ±as (< 0.0001)**: Pueden ser por redondeo, generalmente aceptables
2. **Diferencias grandes**: Indica un posible bug en la reimplementaciÃ³n

Para investigar:
```python
# Cargar y comparar manualmente
import json

with open('tests/baselines/baseline_gold_pred.json') as f:
    baseline = json.load(f)

# Ejecutar nuevo
from grapheval import GraphEvaluator
from rdflib import Graph

test = Graph().parse('data/pred.nt', format='nt')
ref = Graph().parse('data/gold.nt', format='nt')
evaluator = GraphEvaluator(test, ref)
new_results = evaluator.evaluate_all()

# Comparar especÃ­fico
print("Baseline:", baseline['triples'])
print("New:", new_results['triples'])
```

## ğŸ“Š Casos de Prueba

### Caso 1: gold_vs_pred
- **Test**: `data/pred.nt`
- **Reference**: `data/gold.nt`
- **DescripciÃ³n**: ComparaciÃ³n estÃ¡ndar contra gold standard

### Caso 2: gold2_vs_pred
- **Test**: `data/pred.nt`
- **Reference**: `data/gold2.nt`
- **DescripciÃ³n**: ComparaciÃ³n contra gold standard alternativo

## ğŸ”„ Regenerar Baselines

Si modificas la implementaciÃ³n antigua o necesitas actualizar los baselines:

```bash
# Eliminar baselines existentes
rm -rf tests/baselines/

# Regenerar
python tests/generate_baseline.py
```

âš ï¸ **Advertencia**: Solo regenera baselines si estÃ¡s seguro de que la implementaciÃ³n antigua es correcta.

## âœ… Checklist de MigraciÃ³n

Antes de considerar completa la migraciÃ³n:

- [ ] Baselines generados exitosamente
- [ ] Todos los tests de regresiÃ³n pasan
- [ ] No hay mÃ©tricas faltantes importantes
- [ ] Diferencias documentadas y justificadas
- [ ] Performance aceptable (nuevo vs antiguo)

## ğŸ“ Notas

- Los baselines se guardan en formato JSON para fÃ¡cil inspecciÃ³n
- Los tests usan tolerancia de `1e-6` para comparaciones numÃ©ricas
- Las mÃ©tricas basadas en jerarquÃ­as pueden no estar en todos los baselines si falta la ontologÃ­a
